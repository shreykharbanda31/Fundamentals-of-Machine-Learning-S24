{"cells":[{"cell_type":"markdown","metadata":{"id":"CrcWUotASPjV"},"source":["# Reinforcement Learning\n","\n","# Q-Learning\n","\n","This notebook presents SARSA and Q-learning.\n","\n","Credits: T. Bonald, Telecom Paris"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8Lyop7ySPjZ"},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import sys\n","PATH = ''"]},{"cell_type":"code","source":["# This cell has to be run ONLY if you are using google colab on google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","PATH = \"/content/drive/MyDrive/Colab Notebooks/RL/ENSAI-smart-data/\" #Put here the correct path\n","sys.path.append(PATH)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhDpgpEOSXUS","executionInfo":{"status":"ok","timestamp":1673819439019,"user_tz":-60,"elapsed":2195,"user":{"displayName":"Pascal Bianchi","userId":"14431080439002320783"}},"outputId":"3ecf9a34-56e3-455e-ff03-e35aa539474b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zk8CdmfKSPjb"},"outputs":[],"source":["from model import Walk, Maze, TicTacToe, Nim, ConnectFour\n","from agent import Agent, OnlineControl"]},{"cell_type":"markdown","source":["## Handling (state,action) value functions\n","We first make some basic experiments using the walk environment"],"metadata":{"id":"xqrhtfGFSUm9"}},{"cell_type":"code","source":["walk = Walk()"],"metadata":{"id":"-yNUcO0l_z2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the OnlineControl class in agent.py\n","control = OnlineControl(walk)"],"metadata":{"id":"-nPO1yuIPJUM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate a idle (randomly chosen) (state,action) value function\n","for state in walk.get_states():\n","  for action in walk.get_actions(state):\n","    control.state_action_value[walk.encode(state)][action] = # Your code here\n"],"metadata":{"id":"4--RBQPkPWqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Choose an arbitrary state, display the values of the actions : what is the corresponding best action?\n","state = walk.get_states()[2]\n","# Your code here"],"metadata":{"id":"9KziYwUfMtL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For the same state, compute the best action using get_best_action\n","# Your code here"],"metadata":{"id":"F1GHCY1rQvUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For the same state, compute the epsilon greedy best action for a given epsilon\n","control.eps = #Choose epsilon value\n","control.get_best_action_randomized(state)\n","# Discuss the impact of epsilon"],"metadata":{"id":"yigaUiiyEt8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sUnnCBYNSPjb"},"source":["## SARSA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWwpRCuxSPjc"},"outputs":[],"source":["class SARSA(OnlineControl):\n","    \"\"\"Online control by SARSA.\"\"\"\n","        \n","    def update_values(self):\n","        \"\"\"Learn the state-action value online.\"\"\"\n","        self.environment.reinit_state()\n","        state = self.environment.state\n","        action = self.get_best_action_randomized(state)\n","        self.add_state_action(state, action)\n","        for t in range(self.n_steps):\n","            state_code = self.environment.encode(state)\n","            self.state_action_count[state_code][action] += 1\n","            reward, stop = self.environment.step(action)\n","            if stop:\n","                gain = reward\n","            else:\n","                new_state = self.environment.state\n","                new_action = self.get_best_action_randomized(new_state) \n","                self.add_state_action(new_state, new_action)\n","                # to be modified\n","                gain = 0\n","            # to be modified\n","            diff = 0\n","            self.state_action_value[state_code][action] += diff / self.state_action_count[state_code][action]\n","            if stop:\n","                break\n","            state = self.environment.state\n","            action = new_action"]},{"cell_type":"markdown","metadata":{"id":"l8IuihuHSPjc"},"source":["## Q-learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eu1fCH1SPjd"},"outputs":[],"source":["class QLearning(OnlineControl):\n","    \"\"\"Online control by Q-learning.\"\"\"\n","        \n","    def update_values(self):\n","        \"\"\"Learn the state-action value online.\"\"\"\n","        # to be completed\n","        "]},{"cell_type":"markdown","metadata":{"id":"fsV0bUZqSPje"},"source":["## Walk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZL8v0IDSPjf"},"outputs":[],"source":["walk = Walk()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WMPQAlW8SPjf"},"outputs":[],"source":["algo = SARSA(walk, gamma=0.9, eps=0.1, n_steps=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45bbBzBuSPjg"},"outputs":[],"source":["n_episodes = 100\n","for t in range(n_episodes):\n","    algo.update_values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ErPVpbVfSPjg"},"outputs":[],"source":["policy = algo.get_policy()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"IHFVHYhDSPjh"},"outputs":[],"source":["walk.display_policy(policy)"]},{"cell_type":"markdown","metadata":{"id":"-lZG1LE0SPji"},"source":["## Maze"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbFpPtXoSPji"},"outputs":[],"source":["maze = Maze()\n","# set parameters\n","maze_map = np.load(PATH+'maze_small.npy')\n","maze.set_parameters(maze_map, (1, 0), [(3, 8)])\n","# init\n","maze = Maze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oYredZrhSPji"},"outputs":[],"source":["# display the maze"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9NnWbLjSPjj"},"outputs":[],"source":["# Run SARSA and/or Qlearning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZ-PwvmGSPjj"},"outputs":[],"source":["# display the policy"]},{"cell_type":"code","source":["# COMPLETE AT HOME: Run an episode with your policy and show the animation. Can you escape the maze? "],"metadata":{"id":"HIMLu0npYOH7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xdC0uhHvSPjj"},"source":["## Tic-Tac-Toe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y703Td3cSPjk"},"outputs":[],"source":["game = TicTacToe()\n","agent = Agent(game)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHDO_SPPSPjk"},"outputs":[],"source":["# Explain the following line of code, and explain the result: what does it show?\n","np.unique(agent.get_gains(), return_counts=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TJxQVFfUSPjk"},"outputs":[],"source":["# Run SARSA or Q-learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gf-_SgBHSPjl"},"outputs":[],"source":["# Discuss the output of the following code\n","np.unique(agent.get_gains(), return_counts=True)"]},{"cell_type":"code","source":["# Play against the one-step best policy defined in the first lab. Who's winning?\n"],"metadata":{"id":"LDsVARU3VNeq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZD-rlTMSPjl"},"source":["## Perfect adversary\n","\n","Let's get a perfect adversary by Value Iteration."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kk55iSHXSPjl"},"outputs":[],"source":["from scipy import sparse\n","from dp import PolicyEvaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8O4PFMkSPjm"},"outputs":[],"source":["def dot_max(matrix: sparse.csr_matrix, vector: np.ndarray):\n","    \"\"\"Get the dot_max product of a matrix by a vector, replacing the sum by the max.\"\"\"\n","    return np.maximum.reduceat(vector[matrix.indices] * matrix.data, matrix.indptr[:-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUQAGR45SPjm"},"outputs":[],"source":["class ValueIteration(PolicyEvaluation):\n","    \"\"\"Value iteration.\n","    \n","    Parameters\n","    ----------\n","    environment: \n","        The environment.\n","    player: \n","        Player for games (1 or -1, default = 1).\n","    gamma:\n","        Discount factor (between 0 and 1).\n","    n_iter:\n","        Number of value iterations.\n","    tol:\n","        Tolerance = maximum difference between two iterations for early stopping.\n","    \"\"\"\n","    \n","    def __init__(self, environment, player=1, gamma=1, n_iter=100, tol=0, verbose=True):\n","        agent = Agent(environment, player=player)\n","        policy = agent.policy\n","        super(ValueIteration, self).__init__(environment, policy, player, gamma)  \n","        self.n_iter = n_iter\n","        self.tol = tol\n","        self.verbose = verbose\n","        \n","   \n","    def get_optimal_policy(self):\n","        \"\"\"Get the optimal policy by iteration of Bellman's optimality equation.\"\"\"\n","        if hasattr(self.environment, 'player'):\n","            return self.get_optimal_policy_game()\n","        self.values = np.zeros(self.n_states)\n","        moves = self.get_transitions().astype(bool)\n","        for t in range(self.n_iter):\n","            values = self.values.copy()\n","            values_next = self.rewards + self.gamma * self.values\n","            values[self.non_terminal] = dot_max(moves[self.non_terminal], values_next)\n","            diff = np.max(np.abs(values - self.values))\n","            self.values = values\n","            if diff <= self.tol:\n","                if self.verbose:\n","                    print(f\"Convergence after {t+1} iterations.\")\n","                break\n","        policy = self.get_policy()\n","        return policy\n","    \n","    def get_optimal_policy_game(self):\n","        \"\"\"Get the optimal policy for games, assuming the best response of the adversary.\"\"\"\n","        self.values = np.zeros(self.n_states)\n","        moves = self.get_transitions().astype(bool)\n","        player = np.array([state[0] == self.player for state in self.states]) & self.non_terminal\n","        adversary = np.array([state[0] == -self.player for state in self.states]) & self.non_terminal\n","        for t in range(self.n_iter):\n","            values = self.values.copy()\n","            values_next = self.rewards + self.gamma * self.values\n","            values[player] = self.player * dot_max(moves[player], self.player * values_next)\n","            values[adversary] = -self.player * dot_max(moves[adversary], -self.player * values_next)\n","            diff = np.max(np.abs(values - self.values))\n","            self.values = values\n","            if diff <= self.tol:\n","                if self.verbose:\n","                    print(f\"Convergence after {t+1} iterations.\")\n","                break\n","        policy = self.get_policy()\n","        return policy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDr5ckQrSPjn","executionInfo":{"status":"ok","timestamp":1673819467128,"user_tz":-60,"elapsed":19604,"user":{"displayName":"Pascal Bianchi","userId":"14431080439002320783"}},"outputId":"c22e3c60-e7fa-4a4f-ca3f-2de67cec4ec2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Convergence after 6 iterations.\n"]}],"source":["# get a perfect adversary\n","game = TicTacToe(play_first=False, player=-1)\n","algo = ValueIteration(game, player=-1, n_iter=10)\n","adversary_policy = algo.get_optimal_policy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YQC5gAUhSPjn"},"outputs":[],"source":["# Define a TicTacToe game with this player\n","game = TicTacToe(adversary_policy)\n","agent = Agent(game)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OgN-QOWpSPjn","executionInfo":{"status":"ok","timestamp":1673819467452,"user_tz":-60,"elapsed":329,"user":{"displayName":"Pascal Bianchi","userId":"14431080439002320783"}},"outputId":"75a8cb4a-6137-4539-8d95-2afdd7d20fb2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([-1,  0]), array([84, 16]))"]},"metadata":{},"execution_count":35}],"source":["# before training : how many times do the random agent wins/get a draw?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8dxM8pJSPjn"},"outputs":[],"source":["# Now train the agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QssQZ5OASPjo"},"outputs":[],"source":["# After training, evaluate the performance of the agent (frequency of wins/draw)"]},{"cell_type":"markdown","metadata":{"id":"AIPaGmTWSPjo"},"source":["## Nim"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nLysj944SPjp"},"outputs":[],"source":["# Play the Nim game with RL agent, evaluate the improvement over the random agent."]},{"cell_type":"markdown","metadata":{"id":"SYUpYUZlSPjr"},"source":["## Connect Four"]},{"cell_type":"code","source":["# Play the Nim game with RL agent, evaluate the improvement over the random agent."],"metadata":{"id":"e0EZmWdfasEy"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"15lkYG3K-MDiEZFAa2JRiPoW90vAwNf8j","timestamp":1673952468759}]}},"nbformat":4,"nbformat_minor":0}