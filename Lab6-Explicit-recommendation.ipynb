{"cells":[{"cell_type":"markdown","metadata":{"id":"_u5d6HDZ368z"},"source":["# Recommender system: the MovieLens case\n","We are interested in the MovieLens-100k database. https://grouplens.org/datasets/movielens/\n","\n","This notebook is partly inspired by https://github.com/m2dsupsdlclass/lectures-labs/blob/master/labs/03_neural_recsys/Explicit_Feedback_Neural_Recommender_System_rendered.ipynb\n","\n","\n","## 1. Data analysis, visualization and enrichment\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rEEAIyhW3685"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","\n","import os.path as op\n","\n","from zipfile import ZipFile\n","try:\n","    from urllib.request import urlretrieve\n","except ImportError:  # Python 2 compat\n","    from urllib import urlretrieve\n","\n","\n","ML_100K_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n","ML_100K_FILENAME = ML_100K_URL.rsplit('/', 1)[1]\n","ML_100K_FOLDER = 'ml-100k'\n","\n","if not op.exists(ML_100K_FILENAME):\n","    print('Downloading %s to %s...' % (ML_100K_URL, ML_100K_FILENAME))\n","    urlretrieve(ML_100K_URL, ML_100K_FILENAME)\n","\n","if not op.exists(ML_100K_FOLDER):\n","    print('Extracting %s to %s...' % (ML_100K_FILENAME, ML_100K_FOLDER))\n","    ZipFile(ML_100K_FILENAME).extractall('.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGyrTcvX368_"},"outputs":[],"source":["df = pd.read_csv('ml-100k/u.data', sep='\\t', names=['user_id', 'item_id', 'rating', 'timestamp'])\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"hmf-al4t369E"},"source":["<font color=\"red\">With the describe() method, analyze the ratings column.\n","\n","---\n","\n","How many ratings in total? Min and max values? Average, etc.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rth5TyRX369F"},"outputs":[],"source":["df['rating'].describe()"]},{"cell_type":"markdown","metadata":{"id":"dElDGnXK369J"},"source":["<font color=\"red\">\n","Evaluate the number of unique users, the number of unique items.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMy3oRyL369J"},"outputs":[],"source":["# Nombre d'utilisateurs, nombre de films\n","n_user = df['user_id'].nunique()\n","n_item = df['item_id'].nunique()\n","print('n_user =',n_user)\n","print('n_item =',n_item)"]},{"cell_type":"markdown","metadata":{"id":"K9uREt5v369O"},"source":["### Item metadata: data enrichment\n","\n","The item metadata file contains metadata such as the name of the movie or the date it was released. The movies file contains columns indicating the genres of the movie. We only load the first five columns of the file with `usecols`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Up6Ler3l369O"},"outputs":[],"source":["m_cols = ['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n","items = pd.read_csv('ml-100k/u.item', sep='|',names=m_cols, usecols=range(5), encoding='latin-1')\n","items.head()"]},{"cell_type":"markdown","metadata":{"id":"Y_A4BvDI369R"},"source":["We extract the release date as an integer value:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQ0IG1Vx369T"},"outputs":[],"source":["def extract_year(release_date):\n","    if hasattr(release_date, 'split'):\n","        components = release_date.split('-')\n","        if len(components) == 3:\n","            return int(components[2])\n","    # Missing value marker\n","    return 1920\n","\n","items['release_year'] = items['release_date'].map(extract_year)\n","items.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5h2gdmopXgpS"},"outputs":[],"source":["items.hist('release_year',bins=50)"]},{"cell_type":"markdown","metadata":{"id":"wZIPmpL-369W"},"source":["Let's further enrich the data by adding the popularity of each movie"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94h2vggl369a"},"outputs":[],"source":["popularity = df.groupby('item_id').size().reset_index(name='popularity')\n","items = pd.merge(popularity, items)\n","items.head()"]},{"cell_type":"markdown","metadata":{"id":"GsKwmLPk369d"},"source":["We enrich the raw ratings with the metadata collected in this way: we will use this metadata later in this lab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLY1wXrq369e"},"outputs":[],"source":["all_ratings = pd.merge(items, df)\n","all_ratings.head()"]},{"cell_type":"markdown","metadata":{"id":"fkyR9xU_369i"},"source":["<font color=\"red\">\n","\n","*   Élément de liste\n","*   Élément de liste\n","\n","\n","Using groupby, create a new dataframe 'rating_movies' containing the average rating of each movie.\n","Rank the movies by decreasing order of ratings. What are the movies with the highest grades?</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AmI8m3zp369j"},"outputs":[],"source":["rating_movies = pd.DataFrame(all_ratings.groupby('title')['rating'].mean())\n","rating_movies.sort_values(by = 'rating',ascending=False).head(10)"]},{"cell_type":"markdown","metadata":{"id":"qP4chU-I369q"},"source":["<font color=\"red\">\n","Represent the histogram of the average ratings of the films. What do the \"peaks\" at 1 and 5 correspond to?</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CICOpW9c369r"},"outputs":[],"source":["rating_movies.hist(bins=50);"]},{"cell_type":"markdown","metadata":{"id":"R8p7beLJ369x"},"source":["<font color=\"red\">\n","Add to the dataframe 'rating_movies' a new column 'num of ratings' containing the number of rating of every movie. Provide your comments.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k5QrMYrL369y"},"outputs":[],"source":["rating_movies['num of ratings'] = all_ratings.groupby('title')['rating'].count()\n","rating_movies.sort_values(by='rating',ascending=False).head()"]},{"cell_type":"markdown","metadata":{"id":"LCCuyEXP3695"},"source":["Cela confirme que les films les mieux notés ont peu de notes."]},{"cell_type":"markdown","metadata":{"id":"P9i0Suvh3698"},"source":["<font color=\"red\">\n","Display the titles of films with the most ratings and discuss their ratings.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YboGmTjq369-"},"outputs":[],"source":["rating_movies.sort_values(by='num of ratings',ascending=False).head()"]},{"cell_type":"markdown","metadata":{"id":"qAue6f5w36-N"},"source":["<font color=\"red\">\n","With sns.jointplot, represent the point cloud (average rating, number of ratings) on all the films (use the alpha= parameter of your choice).\n","Discuss the graph.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnDYrrhT36-O"},"outputs":[],"source":["import seaborn as sns\n","sns.jointplot(x='rating',y='num of ratings',data=rating_movies,alpha=0.5);"]},{"cell_type":"markdown","metadata":{"id":"zlF2VKVq36-R"},"source":["- Movies with an average rating of 1 or 5 have a small number of ratings\n","- Trend: films with a large number of ratings are globally highly rated films\n","- A frequently rated film cannot have a rating very close to 5"]},{"cell_type":"markdown","metadata":{"id":"QPFYON7B36-R"},"source":["## 2. Evaluation of similarity between items\n","In this part, we set an item, say 'Star Wars (1977)'. We want to find films \"similar\" to Star Wars, and classify them in order of similarity, in order to recommend them.\n","<font color=\"red\">\n","With df.pivot_table, create the interaction matrix, whose indices are the rows corresponding to the user_id, the columns correspond to the item_id, and whose entries are the ratings.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UykWaMS736-S"},"outputs":[],"source":["moviemat = pd.pivot_table(data=all_ratings,index='user_id',columns='title',values='rating')\n","moviemat.head()"]},{"cell_type":"markdown","metadata":{"id":"D6RF1P2J36-c"},"source":["<font color=\"red\">\n","With the .corr method, calculate the correlation of the column-vector 'Star Wars (1977)' with the column 'Liar Liar (1997)'\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j94gEY6536-d"},"outputs":[],"source":["starwars_user_ratings = moviemat['Star Wars (1977)']\n","liarliar_user_ratings = moviemat['Liar Liar (1997)']\n","starwars_user_ratings.corr(liarliar_user_ratings)"]},{"cell_type":"markdown","metadata":{"id":"VibQL2wY36-h"},"source":["<font color=\"red\">\n","What does the result of the corrwith() method below provide?\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phTKUZm736-i"},"outputs":[],"source":["moviemat.corrwith(starwars_user_ratings)"]},{"cell_type":"markdown","metadata":{"id":"OacCkfww36-r"},"source":["<font color=\"red\">\n","Transform the series above into a dataframe, and display the movie titles in ascending order of their correlation with starwars.\n","Discuss the result.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Vhn45Kf36-t"},"outputs":[],"source":["similar_to_starwars = pd.DataFrame(moviemat.corrwith(starwars_user_ratings),columns=['Corr'])\n","similar_to_starwars.sort_values(by='Corr',ascending=False).head(10)"]},{"cell_type":"markdown","metadata":{"id":"S-I4AvXV36-x"},"source":["The problem is that the correlations are calculated on the non-NaN values\n","\n","If a film received an r rating from a single user, and that user rated r starwars, the correlation is 1\n","\n","To avoid this, we will look for the correlation among the films with more than 100 ratings"]},{"cell_type":"markdown","metadata":{"id":"tlJzAIUB36-y"},"source":["<font color=\"red\">\n","\n","Add to the similar_to_starwars dataframe a new column corresponding to the number of ratings of each movie.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-umBqFn36-y"},"outputs":[],"source":["similar_to_starwars['num of ratings'] = rating_movies['num of ratings']\n","similar_to_starwars.head()"]},{"cell_type":"markdown","metadata":{"id":"1oUsPNfB36-2"},"source":["<font color=\"red\">\n","Filter the rows to display only titles with a \"sufficient\" number of ratings for you to retain them in the ranking.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGvuNOpR36-5"},"outputs":[],"source":["most_similar_movies = similar_to_starwars[similar_to_starwars['num of ratings']>100]\n","most_similar_movies.sort_values(by='Corr',ascending=False,inplace=True)\n","most_similar_movies.head(10)"]},{"cell_type":"markdown","metadata":{"id":"vxv9oTrw36_B"},"source":["<font color=\"red\">\n","If this is not done, perform the ranking by decreasing correlation values in \"inplace\" mode.\n","Then, thanks to reset_index, get a new column corresponding to the rating of each movie.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uEew40zD36_C"},"outputs":[],"source":["most_similar_movies.reset_index(inplace=True)\n","most_similar_movies.head(10)"]},{"cell_type":"markdown","metadata":{"id":"5wGFRc0j36_Q"},"source":["## 3. Collaborative filtering\n","We want to predict unobserved ratings from the n_user x n_items interaction matrix\n","<br>\n","<font color=\"red\">\n","- Replace all NaNs with zeros with the fillna(0) method\n","- Extract values as numpy.array\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUFtnrIk36_R"},"outputs":[],"source":["ratings = moviemat.fillna(0).values\n","ratings[:5,:5]"]},{"cell_type":"markdown","metadata":{"id":"XybCY-Fb36_U"},"source":["<font color=\"red\">\n","\n","\n","How many non-zero elements are there? Evaluate the percentage of non-zero elements in this matrix.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMae1S2636_V"},"outputs":[],"source":["(ratings>0).sum()/(ratings.shape[0] * ratings.shape[1])*100"]},{"cell_type":"markdown","metadata":{"id":"83lRhKSb36_X"},"source":["We compute a similarity matrix $S=(S_{i,j})$ between users.\n","We consider the 'centered cosine similarity':\n","$$\n","S_{i,j} = \\frac{\\langle \\bar r_i,\\bar r_j\\rangle}{\\|\\bar r_i\\|\\,\\|\\bar r_j\\|}\n","$$\n","where $\\bar r_i$ is the vector of user ratings $i$, recentered by the average of the ratings given by the user in question. The following function provides the similarity matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zWPX88EF36_Y"},"outputs":[],"source":["def phi(x):\n","    return np.maximum(x,0)\n","\n","def similarity(ratings):\n","\n","    # vector containing for each user the number of ratings given\n","    r_user = (ratings>0).sum(axis=1)\n","\n","    # vector containing for each user the average of the ratings given\n","    m_user = np.divide(ratings.sum(axis=1) , r_user, where=r_user!=0)\n","\n","    # Notes recentered by the average per user: each line i contains the vector \\bar r_i\n","    ratings_ctr = ratings.T - ((ratings.T!=0) * m_user)\n","    ratings_ctr = ratings_ctr.T\n","\n","    # Gram matrix containing inner products\n","    sim = ratings_ctr.dot(ratings_ctr.T)\n","\n","    # Renormalization\n","    norms = np.array([np.sqrt(np.diagonal(sim))])\n","    sim = sim / norms / norms.T\n","    sim = phi(sim)\n","\n","    return sim"]},{"cell_type":"markdown","metadata":{"id":"qI6HPKKq36_d"},"source":["### Prédiction\n","On souhaite prédire toutes les notes d'un utilisateur $u$, à partir des notes données par les utilisateurs qui lui sont similaires.\n","\n","Une première approche consiste, pour tout item $i$, à définir\n","$$\n","\\hat r_{u,i} = \\frac{\\sum_{v} S_{u,v} r_{v,i}}{\\sum_{v:r_{v,i}\\neq 0} S_{u,v}}\n","$$\n","où la somme est restreinte aux utilisateurs $v$ ayant effectivement noté l'item i\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"EtcQn4-636_g"},"source":["<font color=\"red\">\n","Compute the predicted ratings\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YoRTTQI36_j"},"outputs":[],"source":["sim = similarity(ratings)\n","numerator = sim.dot(ratings)\n","denominator = sim.dot(ratings>0)\n","pred_ratings = np.divide(numerator,denominator,where = denominator!=0)"]},{"cell_type":"markdown","metadata":{"id":"YPQwK5Ou36_n"},"source":["<font color=\"red\">\n","Afficher les prédictions du premier utilisateur pour les dix premiers items.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hUaU_uXT36_p"},"outputs":[],"source":["print(pred_ratings[0,:10])"]},{"cell_type":"markdown","metadata":{"id":"v92mB0lR36_t"},"source":["We can assess the error. The metric traditionally used is the RMSE:\n","$$\n","RMSE = \\sqrt{\\frac 1N\\sum_{(u,i)\\text{ observed}}(R_{u,i}-\\hat R_{u,i})^2}\n","$$\n","where $N$ is the number of observed ratings, or the MAE\n","$$\n","MAE = \\frac 1N\\sum_{(u,i)\\text{ observed}}|R_{u,i}-\\hat R_{u,i}|\\,.\n","$$\n","<font color=\"red\">\n","Calculate the RMSE (root mean square error) and the MAE (mean absolute error).</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZhQaX1D36_u"},"outputs":[],"source":["RMSE = np.sqrt(np.sum(((ratings - pred_ratings) * (ratings>0))**2) / np.sum(ratings>0))\n","MAE = np.sum(np.abs((ratings - pred_ratings) * (ratings>0))) / np.sum(ratings>0)\n","RMSE,MAE"]},{"cell_type":"markdown","metadata":{"id":"bPJEqzX636_w"},"source":["### Validation\n","<font color=\"red\">Make a train test split</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_e4EhVB36_y"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train_ratings, test_ratings = train_test_split(all_ratings, test_size=0.2, random_state=0)\n","\n","user_id_train = train_ratings['user_id']\n","item_id_train = train_ratings['item_id']\n","rating_train = train_ratings['rating']\n","\n","user_id_test = test_ratings['user_id']\n","item_id_test = test_ratings['item_id']\n","rating_test = test_ratings['rating']"]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Xkeo1DhS36_z"},"source":["We generate the train and the test rating matrices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDw48cej36_0"},"outputs":[],"source":["from scipy.sparse import *\n","train = coo_matrix((rating_train.values,(user_id_train.values-1,item_id_train.values-1)),\n","                   shape=(n_user,n_item)).toarray()\n","test = coo_matrix((rating_test.values,(user_id_test.values-1,item_id_test.values-1)),\n","                   shape=(n_user,n_item)).toarray()"]},{"cell_type":"markdown","metadata":{"id":"KlGIAGX036_3"},"source":["Nous définissons les fonctions nécessaires à la prédiction et l'évaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcQt63Cs36_4"},"outputs":[],"source":["def predict_ratings(ratings,sim):\n","\n","    wsum_sim = np.abs(sim).dot(ratings>0)\n","    return np.divide(sim.dot(ratings) , wsum_sim, where= wsum_sim!=0)\n","\n","def rmse(ratings,pred):\n","    return np.sqrt(np.sum(((ratings - pred) * (ratings>0))**2) / np.sum(ratings>0))"]},{"cell_type":"markdown","metadata":{"id":"UPB6L3jw36_9"},"source":["<font color=\"red\">\n","- With similarity(), evaluate the similarity matrix on the train set.\n","- Predict ratings\n","- Calculate the RMSE on the test set.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5PN5G7i36__"},"outputs":[],"source":["sim = similarity(train)\n","pred_ratings = predict_ratings(train,sim)\n","rmse(test,pred_ratings)"]},{"cell_type":"markdown","metadata":{"id":"nL_A_gsq37AE"},"source":["### Comparison\n","<font color=\"red\">\n","Predict each rating by the average user rating.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7yBFhpUW37AF"},"outputs":[],"source":["av_ratings = train.sum(axis=1) / (train>0).sum(axis=1)\n","rmse(test,av_ratings.reshape(train.shape[0],1))"]},{"cell_type":"markdown","metadata":{"id":"1Yc6NNlF37AJ"},"source":["### Bias-subtracted Collaborative Filtering\n","Some users are likely to give ratings that are always quite high, or always quite low. There is therefore a bias relative to this user. One can imagine that the relative difference of the notes is more important than their absolute value.\n","\n","So we'll subtract each user's rating average before summing over all similar users, then we'll re-add the subtracted average at the end:\n","$$\n","\\hat r_{u,i} = \\bar r_u + \\frac{\\sum_{v} S_{u,v} (r_{v,i}- \\bar r_v)}{\\sum_{v}S_{u, v}1_{r_{v,i}>0}}\n","$$\n","where $\\bar r_u$ is the average rating of user $u$.\n","<br>\n","<font color=\"red\">\n","Observe the difference between the predict_ratings_bias_sub function below and the previous predict_ratings function.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrQfmzrR37AN"},"outputs":[],"source":["def predict_ratings_bias_sub(ratings,sim):\n","\n","    r_user = (ratings>0).sum(axis=1)\n","    m_user = np.divide(ratings.sum(axis=1) , r_user, where=(r_user!=0))\n","    ratings_moyens = np.dot(m_user.reshape(len(m_user),1), np.ones((1,ratings.shape[1])))\n","\n","    wsum_sim = np.abs(sim).dot(ratings>0)\n","    pred = ratings_moyens + np.divide(sim.dot(ratings-(ratings>0)*ratings_moyens),wsum_sim, where= wsum_sim!=0)\n","\n","    return np.minimum(5,np.maximum(1,pred))"]},{"cell_type":"markdown","metadata":{"id":"SKJFAHrc37AQ"},"source":["<font color=\"red\">\n","Evaluate the performance of the new method.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S7yXH-au37AR"},"outputs":[],"source":["sim = similarity(train)\n","pred_ratings = predict_ratings_bias_sub(train,sim)\n","rmse(test,pred_ratings)"]},{"cell_type":"markdown","metadata":{"id":"YAgjIrBw37Al"},"source":["## 4. Matrix factorization approach\n","\n","Rating prediction as a regression problem. We seek to solve the following optimization problem:\n","$$\n","\\min_{P,Q} \\sum_{(u,i)\\,\\text{observed}} (R_{u,i}- (UV)_{u,i})^2 + \\lambda \\|U\\ |^2_F+ \\lambda \\|V\\|^2_F\n","$$\n","Or\n","- $U$ is an array $n_{\\text{user}}\\times K$,\n","- $V$ is a matrix $K\\times n_{\\text{item}}$,\n","- $K$ is an integer, $\\lambda>0$ is a regularization parameter,\n","- $\\|\\,.\\,\\|_F$ represents the Froebenius norm (the root of the sum of the squares of its coefficients).\n","\n","<img src=\"https://bianchi.wp.imt.fr/files/2019/01/rec_archi_1.jpg\" style=\"width: 600px;\" />\n","\n","The optimization problem is non-convex. We can seek to obtain a local minimum of the above criterion. At least two methods are commonly used.\n","\n","Method 1: ALS (alternating least square). At each iteration $t$, we have an estimate $(U_t,V_t)$ of the solution. We successively solve the following subproblems:\n","\\begin{align}\n","& U_{t+1} = \\arg\\min_{U} \\sum_{(u,i)\\,\\text{observed}} (R_{u,i}- (UV_t)_{u,i})^ 2 + \\lambda \\|U\\|^2_F \\\\\n","& V_{t+1} = \\arg\\min_{V} \\sum_{(u,i)\\,\\text{observed}} (R_{u,i}- (U_{t+1}V)_{ u,i})^2 + \\lambda \\|V\\|^2_F\n","\\end{align}\n","These two problems are quadratic and convex and can be easily solved (for example, using a conjugate gradient algorithm). The algorithm converges to a local minimum of the objective function described at the beginning of this paragraph.\n","\n","Method 2: SGD (stochastic gradient algorithm).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmAloKFq37A-"},"outputs":[],"source":["from keras.layers import Input, Embedding, Flatten, Dot\n","from keras.models import Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2zASxK737BC"},"outputs":[],"source":["# For each sample we input the integer identifiers\n","# of a single user and a single item\n","user_id_input = Input(shape=[1],name='user')\n","item_id_input = Input(shape=[1], name='item')\n","\n","embedding_size = 30\n","user_embedding = Embedding(output_dim=embedding_size, input_dim=n_user + 1,\n","                           input_length=1, name='user_embedding')(user_id_input)\n","\n","item_embedding = Embedding(output_dim=embedding_size, input_dim=n_item + 1,\n","                           input_length=1, name='item_embedding')(item_id_input)\n","\n","# reshape from shape: (batch_size, input_length, embedding_size)\n","# to shape: (batch_size, input_length * embedding_size) which is\n","# equal to shape: (batch_size, embedding_size)\n","user_vecs = Flatten()(user_embedding)\n","item_vecs = Flatten()(item_embedding)\n","\n","y = Dot(axes=1)([user_vecs, item_vecs])\n","\n","model = Model(inputs=[user_id_input, item_id_input], outputs=y)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3msWOCA37BE"},"outputs":[],"source":["print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lnlsXiIw37BF"},"outputs":[],"source":["from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file='model.png')\n","from IPython.display import Image\n","Image(filename='model.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPdHLIjA37BH"},"outputs":[],"source":["model.compile(optimizer='adam', loss='mae')"]},{"cell_type":"markdown","metadata":{"id":"rO_V8nc937BJ"},"source":["<font color=\"red\">How are the parameters of the hidden layer of embeddings initialized?\n","<br>\n","In order to verify that the code does not contain errors, predict the ratings when the input is the train set [user_id_train, item_id_train].\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRHWVUSD37BK"},"outputs":[],"source":["# Useful for debugging the output shape of model\n","initial_train_preds = model.predict([user_id_train, item_id_train])\n","initial_train_preds.shape"]},{"cell_type":"markdown","metadata":{"id":"fIn_X6OP37BN"},"source":["<font color=\"red\">Calculate the MSE and MAE on the train set. Is it satisfactory? Why?</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyOlkSHm37BN"},"outputs":[],"source":["squared_differences = np.square(initial_train_preds[:,0] - rating_train.values)\n","absolute_differences = np.abs(initial_train_preds[:,0] - rating_train.values)\n","\n","print(\"Random init MSE: %0.3f\" % np.mean(squared_differences))\n","print(\"Random init MAE: %0.3f\" % np.mean(absolute_differences))\n","\n","# You may also use sklearn metrics to do so using scikit-learn:\n","\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","print(\"Random init MSE: %0.3f\" % mean_squared_error(initial_train_preds, rating_train))\n","print(\"Random init MAE: %0.3f\" % mean_absolute_error(initial_train_preds, rating_train))\n"]},{"cell_type":"markdown","metadata":{"id":"bhZ9ECMr37BQ"},"source":["The following command saves the model parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8c-OM2sv37BS"},"outputs":[],"source":["model.save_weights('initial_weights.h5')"]},{"cell_type":"markdown","metadata":{"id":"nT72_m4L37BT"},"source":["### Model training\n","\n","history.history which is returned by the model.fit function is a dictionary containing the 'loss' and the 'val_loss', the validation loss, after each epoch (one epoch = one pass over the data).\n","<br>\n","<font color=\"red\">Explain what the arguments to model.fit() are below.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eOcYSgf37BW"},"outputs":[],"source":["%%time\n","\n","# Training the model\n","history = model.fit([user_id_train, item_id_train], rating_train,\n","                    batch_size=64, epochs=20, validation_split=0.1,\n","                    shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"mronj5hF37BX"},"source":["<font color=\"red\">Plot a graph representing the train loss and the validation loss.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJPWbaQo37BY"},"outputs":[],"source":["plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='validation')\n","plt.ylim(0, 2)\n","plt.legend(loc='best')\n","plt.title('Loss');"]},{"cell_type":"markdown","metadata":{"id":"YQM47Pse37BZ"},"source":["<font color=\"red\">Why is train loss greater than validation loss in early iterations?\n","<br>\n","Compute predictions on test set [user_id_test, item_id_test] using model.predict(..)<br>\n","Evaluate MAE and MSE.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oad1VpVW37Ba"},"outputs":[],"source":["test_preds = model.predict([user_id_test, item_id_test])\n","print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n","print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"]},{"cell_type":"markdown","metadata":{"id":"xHA7feg737Bb"},"source":["### Early stopping\n","<br>\n","<font color=\"red\">How does the validation loss behave in the last iterations? How to explain this phenomenon?</font>\n","<br>\n","We want to reproduce the previous experiment by adding a stopping criterion when the validation loss increases.\n","<br>\n","<font color=\"red\">With model.load_weights(...), reset the model to the starting parameters.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTQM_Ibp37Bc"},"outputs":[],"source":["model.load_weights('initial_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrKEunCv37Bd"},"outputs":[],"source":["from keras.callbacks import EarlyStopping\n","early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n","model.fit([user_id_train, item_id_train], rating_train,\n","                    batch_size=64, epochs=20, validation_split=0.1,\n","                    callbacks=[early_stopping], shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8Ez9jtG37Be"},"outputs":[],"source":["test_preds = model.predict([user_id_test, item_id_test])\n","print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n","print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"]},{"cell_type":"markdown","metadata":{"id":"rQK8EFLd37Bf"},"source":["ANSWER THIS QUESTION:\n","\n","<font color=\"red\">Why did we reset the model with model.load_weights('initial_weights.h5')? What would have happened otherwise?</font>\n"]},{"cell_type":"markdown","metadata":{"id":"N-zrc-X-37Bf"},"source":["## Deep recommender model\n","\n","Here is a more complex structure:\n","\n","<img src=\"https://bianchi.wp.imt.fr/files/2019/01/rec_archi_2.jpg\" style=\"width: 600px;\" />\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IB3Ce5T-37Bg"},"outputs":[],"source":["from keras.layers import Concatenate, Dropout, Dense"]},{"cell_type":"markdown","metadata":{"id":"nOYwikhJ37Bh"},"source":["<font color=\"red\">Comment the code below. In particular:<br>\n","- What are the Concatenate, Dropout, Dense functions for?<br>\n","- What non-linearity is used, and where in the network?</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1E_d1su37Bh"},"outputs":[],"source":["user_id_input = Input(shape=[1], name='user')\n","item_id_input = Input(shape=[1], name='item')\n","\n","embedding_size = 30\n","user_embedding = Embedding(output_dim=embedding_size, input_dim=n_user + 1,\n","                           input_length=1, name='user_embedding')(user_id_input)\n","item_embedding = Embedding(output_dim=embedding_size, input_dim=n_item + 1,\n","                           input_length=1, name='item_embedding')(item_id_input)\n","\n","user_vecs = Flatten()(user_embedding)\n","item_vecs = Flatten()(item_embedding)\n","\n","input_vecs = Concatenate()([user_vecs, item_vecs])\n","## Careful: Dropout too high prevents any training\n","input_vecs = Dropout(0.5)(input_vecs)\n","\n","x = Dense(64, activation='relu')(input_vecs)\n","\n","y = Dense(1)(x)\n","\n","model = Model(inputs=[user_id_input, item_id_input], outputs=y)\n","model.compile(optimizer='adam', loss='mae')\n","\n","initial_train_preds = model.predict([user_id_train, item_id_train])\n"]},{"cell_type":"markdown","metadata":{"id":"s8h85XSR37Bk"},"source":["<font color=\"red\">Draw the network, graphically</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"44IRMc_R37Bk"},"outputs":[],"source":["plot_model(model, to_file='model.png')\n","Image(filename='model.png')"]},{"cell_type":"markdown","metadata":{"id":"S6F_QiTv37Bm"},"source":["<font color=\"red\">What is the total number of parameters?</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMDC_I6937Bm"},"outputs":[],"source":["print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"A25eIGKh37Bn"},"source":["<font color=\"red\">Fit the model on the train set and evaluate performance\n","on the test.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2shCCeS37Bn"},"outputs":[],"source":["%%time\n","history = model.fit([user_id_train, item_id_train], rating_train,\n","                    batch_size=64, epochs=20, validation_split=0.1,\n","                    callbacks = [early_stopping], shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9W2-Qvm37Bp"},"outputs":[],"source":["test_preds = model.predict([user_id_test, item_id_test])\n","print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n","print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"]},{"cell_type":"markdown","metadata":{"id":"3Xki7ic737Bv"},"source":["### Exercise (at home)\n","  - Add extra layer, compare performance\n","  - Try to add dropout while modifying the sizes of the different layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWE3Im2h37Bv"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"k1zvHP6237Bw"},"source":["## 6. Viewing embeddings\n","The following command extracts the coefficients of the different layers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_mZfvWa37Bx"},"outputs":[],"source":["weights = model.get_weights()\n","[w.shape for w in weights]"]},{"cell_type":"markdown","metadata":{"id":"XfwClP-S37By"},"source":["<font color=\"red\">Extract the matrix of user embeddings, and that of items.<br>\n","Display the embedding of item 0.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rrny6dT37Bz"},"outputs":[],"source":["user_embeddings = weights[0]\n","item_embeddings = weights[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN8m1Bp937B0"},"outputs":[],"source":["print(\"First item name from metadata:\", items[\"title\"][0])\n","print(\"Embedding vector for the first item:\")\n","print(item_embeddings[0])"]},{"cell_type":"markdown","metadata":{"id":"1HofO1wm37B2"},"source":["### Visualisation des embeddings par tSNE <br>\n","<font color=\"red\">Créer un transformer TSNE. On pourra choisir une perplexité égale à 30. Transformer \"item_embeddings\" avec fit_transform.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"717BpsgN37B2"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","tsne = TSNE(perplexity=30)\n","\n","item_tsne = tsne.fit_transform(item_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uUx2MVCi37B2"},"outputs":[],"source":["plt.figure(figsize=(10, 10))\n","plt.scatter(item_tsne[:, 0], item_tsne[:, 1]);\n","plt.xticks(()); plt.yticks(());\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pwFOL2AU37B4"},"source":["Clusters seem (vaguely) to appear. We want to know to which points the films correspond.<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTVWltNw37B4"},"outputs":[],"source":["index_most_popular = items[items['popularity']>200].index\n","title_most_popular = items[items['popularity']>200].title"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oGy2zjSO37B5"},"outputs":[],"source":["from bokeh.models import ColumnDataSource, LabelSet\n","from bokeh.plotting import figure, show, output_file\n","from bokeh.io import output_notebook\n","output_notebook()\n","\n","p = figure(tools=\"pan,wheel_zoom,reset,save\",\n","           toolbar_location=\"above\",\n","           title=\"T-SNE for most popular movies\")\n","\n","source = ColumnDataSource(data=dict(x1=item_tsne[index_most_popular,0],\n","                                    x2=item_tsne[index_most_popular,1],\n","                                    names=title_most_popular))\n","\n","p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n","\n","labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n","                  text_font_size=\"8pt\", text_color=\"#555555\",\n","                  source=source, text_align='center')\n","p.add_layout(labels)\n","\n","show(p)"]},{"cell_type":"markdown","metadata":{"id":"sgiZ_-bm37B6"},"source":["## 7. Incorporate metadata into the template\n","\n","Using a framework similar to the one used previously, we will build another in-depth model that can also leverage additional metadata. The resulting system is therefore a **hybrid recommender system** that performs both **collaborative filtering** and **content-based recommendations**.\n","<img src=\"images/rec_archi_3.svg\" style=\"width: 600px;\" />"]},{"cell_type":"markdown","metadata":{"id":"c4WQGoZ937B7"},"source":["We want to add the columns ['popularity', 'release_year'] as input to our regressor, in addition to user_id and item_id. We pre-process these columns.\n","\n","The QuantileTransformer method transforms a feature so that the output follows a uniform distribution. The \"fit\" therefore consists in calculating the (empirical) distribution function of the input sequence, and the transform in applying this distribution function sample by sample.<br>\n","\n","<font color=\"red\">What can be the point of this preliminary transformation?</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wk0nd25K37B8"},"outputs":[],"source":["from sklearn.preprocessing import QuantileTransformer\n","\n","meta_columns = ['popularity', 'release_year']\n","\n","scaler = QuantileTransformer()\n","item_meta_train = scaler.fit_transform(train_ratings[meta_columns])\n","item_meta_test = scaler.transform(test_ratings[meta_columns])"]},{"cell_type":"markdown","metadata":{"id":"NAkkrQ2i37B9"},"source":["We want to create the following architecture:\n","\n","<img src=\"https://bianchi.wp.imt.fr/files/2019/01/model-metadata.png\" style=\"width: 500px;\" />\n","\n","<font color=\"red\">- What are the main differences with the previous network?<br>\n","- It's up to you to create this model, compile it, train it, and evaluate its performance.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JOYn7I_437B9"},"outputs":[],"source":["user_id_input = Input(shape=[1], name='user')\n","item_id_input = Input(shape=[1], name='item')\n","meta_input = Input(shape=[2], name='meta_item')\n","\n","embedding_size = 32\n","user_embedding = Embedding(output_dim=embedding_size, input_dim=n_user+1,\n","                           input_length=1, name='user_embedding')(user_id_input)\n","item_embedding = Embedding(output_dim=embedding_size, input_dim=n_item+1,\n","                           input_length=1, name='item_embedding')(item_id_input)\n","\n","\n","user_vecs = Flatten()(user_embedding)\n","item_vecs = Flatten()(item_embedding)\n","\n","input_vecs = Concatenate()([user_vecs, item_vecs, meta_input])\n","\n","x = Dense(64, activation='relu')(input_vecs)\n","x = Dropout(0.5)(x)\n","x = Dense(32, activation='relu')(x)\n","y = Dense(1)(x)\n","\n","model = Model(inputs=[user_id_input, item_id_input, meta_input], outputs=y)\n","model.compile(optimizer='adam', loss='mae')\n","\n","initial_train_preds = model.predict([user_id_train, item_id_train, item_meta_train])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hxONEhfM37B-"},"outputs":[],"source":["plot_model(model, to_file='model.png')\n","Image(filename='model.png')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K85U8wMh37B_"},"outputs":[],"source":["history = model.fit([user_id_train, item_id_train, item_meta_train], rating_train,\n","                    batch_size=64, epochs=25, validation_split=0.1,\n","                    callbacks = [early_stopping],shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VaMKFnad37CA"},"outputs":[],"source":["test_preds = model.predict([user_id_test, item_id_test, item_meta_test])\n","print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n","print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1HTJcCz37CC"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["uj5SBmRn37A5"],"provenance":[{"file_id":"1PvTfXDuEZ_kM4BJ6GmYwMb9WFKnpYaVG","timestamp":1681983061947}]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
